2024-01-25 05:05:16,890 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]
CUDA available: True
GPU 0: Tesla T4
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 12.2, V12.2.140
GCC: x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
PyTorch: 1.11.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0+cu113
OpenCV: 4.8.0
MMCV: 1.5.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.20.0+f3aac93
------------------------------------------------------------

2024-01-25 05:05:17,633 - mmdet - INFO - Distributed training: True
2024-01-25 05:05:18,374 - mmdet - INFO - Config:
model = dict(
    type='RetinaNet',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(
            type='Pretrained',
            checkpoint=
            '/content/drive/MyDrive/PPALVuongTan/PPAL/work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8/epoch_20.pth'
        )),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_input',
        num_outs=5),
    bbox_head=dict(
        type='RetinaQualityEMAHead',
        num_classes=10,
        in_channels=256,
        stacked_convs=4,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            octave_base_scale=4,
            scales_per_octave=3,
            ratios=[0.5, 1.0, 2.0],
            strides=[8, 16, 32, 64, 128]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0),
        base_momentum=0.99),
    train_cfg=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.4,
            min_pos_iou=0,
            ignore_iof_thr=-1),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    test_cfg=dict(
        nms_pre=5000,
        min_bbox_size=0,
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.5),
        max_per_img=100))
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[20])
runner = dict(type='EpochBasedRunner', max_epochs=10)
checkpoint_config = dict(interval=5, max_keep_ckpts=1, by_epoch=True)
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
mmdet_base = '../../_base_'
dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file=
        'work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8/annotations/labeled.json',
        img_prefix='data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
labeled_data = 'work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8/annotations/labeled.json'
unlabeled_data = 'work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8/annotations/unlabeled.json'
evaluation = dict(interval=999999, metric='bbox')
work_dir = 'work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8'
auto_resume = False
gpu_ids = range(0, 1)

2024-01-25 05:05:18,375 - mmdet - INFO - Set random seed to 745053175, deterministic: False
2024-01-25 05:05:18,779 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': '/content/drive/MyDrive/PPALVuongTan/PPAL/work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8/epoch_20.pth'}
2024-01-25 05:05:23,628 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2024-01-25 05:05:23,684 - mmdet - INFO - initialize RetinaQualityEMAHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'retina_cls', 'std': 0.01, 'bias_prob': 0.01}}
Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.bn3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.bn3.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.4.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of RetinaNet  

bbox_head.retina_cls.weight - torch.Size([90, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

bbox_head.retina_cls.bias - torch.Size([90]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.retina_reg.bias - torch.Size([36]): 
NormalInit: mean=0, std=0.01, bias=0 
2024-01-25 05:05:24,887 - mmdet - INFO - Start running, host: root@40539756e35f, work_dir: /content/drive/MyDrive/PPALVuongTan/PPAL/work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8
2024-01-25 05:05:24,888 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) NumClassCheckHook                  
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) NumClassCheckHook                  
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2024-01-25 05:05:24,888 - mmdet - INFO - workflow: [('train', 1)], max: 10 epochs
2024-01-25 05:05:24,888 - mmdet - INFO - Checkpoints will be saved to /content/drive/MyDrive/PPALVuongTan/PPAL/work_dirs/retinanet_coco_ppal_5rounds_2percent_to_10percent/round8 by HardDiskBackend.
2024-01-25 05:06:58,676 - mmdet - INFO - Iter [50/11160]	lr: 9.890e-04, eta: 5:47:13, time: 1.875, data_time: 0.602, memory: 6332, loss_cls: 1.1476, loss_bbox: 2.3686, loss: 3.5162, grad_norm: 1.2382
2024-01-25 05:08:08,837 - mmdet - INFO - Iter [100/11160]	lr: 1.988e-03, eta: 5:02:09, time: 1.403, data_time: 0.074, memory: 6332, loss_cls: 1.1392, loss_bbox: 1.5954, loss: 2.7346, grad_norm: 2.0551
2024-01-25 05:09:23,284 - mmdet - INFO - Iter [150/11160]	lr: 2.987e-03, eta: 4:51:36, time: 1.489, data_time: 0.155, memory: 6495, loss_cls: 1.1240, loss_bbox: 1.1700, loss: 2.2940, grad_norm: 1.2343
2024-01-25 05:10:32,395 - mmdet - INFO - Iter [200/11160]	lr: 3.986e-03, eta: 4:40:49, time: 1.382, data_time: 0.075, memory: 6593, loss_cls: 1.1023, loss_bbox: 1.1651, loss: 2.2674, grad_norm: 1.4403
2024-01-25 05:11:47,086 - mmdet - INFO - Iter [250/11160]	lr: 4.985e-03, eta: 4:37:57, time: 1.494, data_time: 0.154, memory: 6593, loss_cls: 1.0253, loss_bbox: 1.1725, loss: 2.1979, grad_norm: 1.7221
2024-01-25 05:12:58,225 - mmdet - INFO - Iter [300/11160]	lr: 5.984e-03, eta: 4:33:29, time: 1.423, data_time: 0.103, memory: 6960, loss_cls: 0.8699, loss_bbox: 1.1235, loss: 1.9934, grad_norm: 1.8688
2024-01-25 05:14:09,413 - mmdet - INFO - Iter [350/11160]	lr: 6.983e-03, eta: 4:29:59, time: 1.424, data_time: 0.103, memory: 6960, loss_cls: 0.8652, loss_bbox: 1.1488, loss: 2.0140, grad_norm: 1.9294
2024-01-25 05:15:19,574 - mmdet - INFO - Iter [400/11160]	lr: 7.982e-03, eta: 4:26:36, time: 1.403, data_time: 0.058, memory: 6960, loss_cls: 0.8531, loss_bbox: 1.1509, loss: 2.0040, grad_norm: 1.6960
2024-01-25 05:16:28,714 - mmdet - INFO - Iter [450/11160]	lr: 8.981e-03, eta: 4:23:18, time: 1.383, data_time: 0.054, memory: 11475, loss_cls: 0.9318, loss_bbox: 1.1020, loss: 2.0338, grad_norm: 1.4849
2024-01-25 05:17:46,171 - mmdet - INFO - Iter [500/11160]	lr: 9.980e-03, eta: 4:23:23, time: 1.549, data_time: 0.233, memory: 11475, loss_cls: 0.8698, loss_bbox: 1.1689, loss: 2.0387, grad_norm: 1.5772
2024-01-25 05:18:59,375 - mmdet - INFO - Iter [550/11160]	lr: 1.000e-02, eta: 4:21:51, time: 1.464, data_time: 0.147, memory: 11475, loss_cls: 0.9158, loss_bbox: 1.1249, loss: 2.0406, grad_norm: 1.1061
2024-01-25 05:20:10,584 - mmdet - INFO - Iter [600/11160]	lr: 1.000e-02, eta: 4:19:47, time: 1.424, data_time: 0.083, memory: 11475, loss_cls: 0.8330, loss_bbox: 1.2036, loss: 2.0366, grad_norm: 0.9376
2024-01-25 05:21:19,373 - mmdet - INFO - Iter [650/11160]	lr: 1.000e-02, eta: 4:17:12, time: 1.376, data_time: 0.051, memory: 11475, loss_cls: 0.8566, loss_bbox: 1.1597, loss: 2.0163, grad_norm: 1.2095
2024-01-25 05:22:30,850 - mmdet - INFO - Iter [700/11160]	lr: 1.000e-02, eta: 4:15:30, time: 1.430, data_time: 0.103, memory: 11475, loss_cls: 0.8398, loss_bbox: 1.1358, loss: 1.9755, grad_norm: 1.2968
2024-01-25 05:23:43,212 - mmdet - INFO - Iter [750/11160]	lr: 1.000e-02, eta: 4:14:04, time: 1.447, data_time: 0.119, memory: 11475, loss_cls: 0.8447, loss_bbox: 1.0922, loss: 1.9369, grad_norm: 1.0840
2024-01-25 05:24:51,965 - mmdet - INFO - Iter [800/11160]	lr: 1.000e-02, eta: 4:11:53, time: 1.375, data_time: 0.037, memory: 11475, loss_cls: 0.8354, loss_bbox: 1.0408, loss: 1.8762, grad_norm: 1.4784
2024-01-25 05:26:04,304 - mmdet - INFO - Iter [850/11160]	lr: 1.000e-02, eta: 4:10:32, time: 1.447, data_time: 0.115, memory: 11475, loss_cls: 0.8326, loss_bbox: 1.0618, loss: 1.8943, grad_norm: 1.4114
2024-01-25 05:27:13,798 - mmdet - INFO - Iter [900/11160]	lr: 1.000e-02, eta: 4:08:41, time: 1.390, data_time: 0.065, memory: 11475, loss_cls: 0.7948, loss_bbox: 1.0307, loss: 1.8255, grad_norm: 1.4142
2024-01-25 05:28:23,095 - mmdet - INFO - Iter [950/11160]	lr: 1.000e-02, eta: 4:06:51, time: 1.386, data_time: 0.060, memory: 11475, loss_cls: 0.8045, loss_bbox: 1.1055, loss: 1.9100, grad_norm: 1.3458
2024-01-25 05:29:37,865 - mmdet - INFO - Exp name: retinanet_26e.py
2024-01-25 05:29:37,865 - mmdet - INFO - Iter [1000/11160]	lr: 1.000e-02, eta: 4:06:01, time: 1.495, data_time: 0.152, memory: 11475, loss_cls: 0.8084, loss_bbox: 1.0597, loss: 1.8680, grad_norm: 1.3715
2024-01-25 05:30:49,647 - mmdet - INFO - Iter [1050/11160]	lr: 1.000e-02, eta: 4:04:40, time: 1.436, data_time: 0.090, memory: 11475, loss_cls: 0.7997, loss_bbox: 1.0336, loss: 1.8332, grad_norm: 1.3491
2024-01-25 05:32:05,030 - mmdet - INFO - Iter [1100/11160]	lr: 1.000e-02, eta: 4:03:53, time: 1.508, data_time: 0.176, memory: 11475, loss_cls: 0.7931, loss_bbox: 1.0007, loss: 1.7938, grad_norm: 1.2322
2024-01-25 05:33:28,401 - mmdet - INFO - Iter [1150/11160]	lr: 1.000e-02, eta: 4:04:24, time: 1.692, data_time: 0.349, memory: 11475, loss_cls: 0.7792, loss_bbox: 0.9978, loss: 1.7770, grad_norm: 1.0411
2024-01-25 05:34:35,015 - mmdet - INFO - Iter [1200/11160]	lr: 1.000e-02, eta: 4:02:15, time: 1.332, data_time: 0.014, memory: 11475, loss_cls: 0.7977, loss_bbox: 1.0324, loss: 1.8301, grad_norm: 1.3595
2024-01-25 05:35:42,334 - mmdet - INFO - Iter [1250/11160]	lr: 1.000e-02, eta: 4:00:18, time: 1.346, data_time: 0.015, memory: 11475, loss_cls: 0.7722, loss_bbox: 1.0608, loss: 1.8330, grad_norm: 1.2815
2024-01-25 05:36:49,689 - mmdet - INFO - Iter [1300/11160]	lr: 1.000e-02, eta: 3:58:24, time: 1.347, data_time: 0.015, memory: 11475, loss_cls: 0.7777, loss_bbox: 0.9992, loss: 1.7769, grad_norm: 1.3285
2024-01-25 05:37:56,818 - mmdet - INFO - Iter [1350/11160]	lr: 1.000e-02, eta: 3:56:32, time: 1.343, data_time: 0.015, memory: 11475, loss_cls: 0.8209, loss_bbox: 1.0092, loss: 1.8301, grad_norm: 1.4343
2024-01-25 05:39:03,879 - mmdet - INFO - Iter [1400/11160]	lr: 1.000e-02, eta: 3:54:43, time: 1.341, data_time: 0.015, memory: 11475, loss_cls: 0.7797, loss_bbox: 1.0169, loss: 1.7965, grad_norm: 1.2788
2024-01-25 05:40:11,352 - mmdet - INFO - Iter [1450/11160]	lr: 1.000e-02, eta: 3:52:59, time: 1.349, data_time: 0.015, memory: 11475, loss_cls: 0.7835, loss_bbox: 0.9685, loss: 1.7520, grad_norm: 1.3111
2024-01-25 05:41:18,556 - mmdet - INFO - Iter [1500/11160]	lr: 1.000e-02, eta: 3:51:17, time: 1.344, data_time: 0.016, memory: 11475, loss_cls: 0.7486, loss_bbox: 0.9508, loss: 1.6994, grad_norm: 1.2389
2024-01-25 05:42:25,808 - mmdet - INFO - Iter [1550/11160]	lr: 1.000e-02, eta: 3:49:36, time: 1.345, data_time: 0.015, memory: 11475, loss_cls: 0.7736, loss_bbox: 0.9988, loss: 1.7724, grad_norm: 1.4366
2024-01-25 05:43:32,319 - mmdet - INFO - Iter [1600/11160]	lr: 1.000e-02, eta: 3:47:54, time: 1.330, data_time: 0.015, memory: 11475, loss_cls: 0.7645, loss_bbox: 1.0335, loss: 1.7980, grad_norm: 1.2350
2024-01-25 05:44:38,887 - mmdet - INFO - Iter [1650/11160]	lr: 1.000e-02, eta: 3:46:14, time: 1.331, data_time: 0.015, memory: 11475, loss_cls: 0.7544, loss_bbox: 0.9873, loss: 1.7417, grad_norm: 1.2777
2024-01-25 05:45:46,287 - mmdet - INFO - Iter [1700/11160]	lr: 1.000e-02, eta: 3:44:40, time: 1.348, data_time: 0.015, memory: 11475, loss_cls: 0.7643, loss_bbox: 1.0337, loss: 1.7980, grad_norm: 1.3710
2024-01-25 05:46:52,681 - mmdet - INFO - Iter [1750/11160]	lr: 1.000e-02, eta: 3:43:03, time: 1.328, data_time: 0.015, memory: 11475, loss_cls: 0.7683, loss_bbox: 0.9959, loss: 1.7642, grad_norm: 1.2676
2024-01-25 05:48:00,595 - mmdet - INFO - Iter [1800/11160]	lr: 1.000e-02, eta: 3:41:35, time: 1.358, data_time: 0.016, memory: 11475, loss_cls: 0.7868, loss_bbox: 0.9963, loss: 1.7831, grad_norm: 1.4309
2024-01-25 05:49:08,131 - mmdet - INFO - Iter [1850/11160]	lr: 1.000e-02, eta: 3:40:07, time: 1.351, data_time: 0.016, memory: 11475, loss_cls: 0.7459, loss_bbox: 0.9889, loss: 1.7348, grad_norm: 1.3500
2024-01-25 05:50:14,194 - mmdet - INFO - Iter [1900/11160]	lr: 1.000e-02, eta: 3:38:32, time: 1.321, data_time: 0.016, memory: 11475, loss_cls: 0.7407, loss_bbox: 0.9985, loss: 1.7392, grad_norm: 1.3507
2024-01-25 05:51:22,139 - mmdet - INFO - Iter [1950/11160]	lr: 1.000e-02, eta: 3:37:08, time: 1.359, data_time: 0.016, memory: 11475, loss_cls: 0.7481, loss_bbox: 0.9666, loss: 1.7146, grad_norm: 1.2765
2024-01-25 05:52:30,011 - mmdet - INFO - Exp name: retinanet_26e.py
2024-01-25 05:52:30,011 - mmdet - INFO - Iter [2000/11160]	lr: 1.000e-02, eta: 3:35:44, time: 1.357, data_time: 0.016, memory: 11475, loss_cls: 0.7435, loss_bbox: 0.9939, loss: 1.7374, grad_norm: 1.3747
2024-01-25 05:53:37,535 - mmdet - INFO - Iter [2050/11160]	lr: 1.000e-02, eta: 3:34:19, time: 1.350, data_time: 0.016, memory: 11475, loss_cls: 0.7371, loss_bbox: 0.9926, loss: 1.7298, grad_norm: 1.2678
2024-01-25 05:54:45,211 - mmdet - INFO - Iter [2100/11160]	lr: 1.000e-02, eta: 3:32:56, time: 1.354, data_time: 0.016, memory: 11475, loss_cls: 0.7581, loss_bbox: 0.9756, loss: 1.7338, grad_norm: 1.4618
2024-01-25 05:55:52,739 - mmdet - INFO - Iter [2150/11160]	lr: 1.000e-02, eta: 3:31:33, time: 1.351, data_time: 0.016, memory: 11475, loss_cls: 0.7580, loss_bbox: 0.9914, loss: 1.7494, grad_norm: 1.3541
2024-01-25 05:56:59,824 - mmdet - INFO - Iter [2200/11160]	lr: 1.000e-02, eta: 3:30:09, time: 1.342, data_time: 0.016, memory: 11475, loss_cls: 0.7733, loss_bbox: 0.9695, loss: 1.7428, grad_norm: 1.3840
2024-01-25 05:58:19,604 - mmdet - INFO - Iter [2250/11160]	lr: 1.000e-02, eta: 3:30:55, time: 1.995, data_time: 0.645, memory: 11475, loss_cls: 0.7320, loss_bbox: 0.9534, loss: 1.6855, grad_norm: 1.1813
2024-01-25 05:59:27,584 - mmdet - INFO - Iter [2300/11160]	lr: 1.000e-02, eta: 3:29:32, time: 1.360, data_time: 0.014, memory: 11475, loss_cls: 0.7649, loss_bbox: 0.9783, loss: 1.7432, grad_norm: 1.5090
2024-01-25 06:00:34,947 - mmdet - INFO - Iter [2350/11160]	lr: 1.000e-02, eta: 3:28:08, time: 1.347, data_time: 0.015, memory: 11475, loss_cls: 0.7275, loss_bbox: 0.9803, loss: 1.7078, grad_norm: 1.4359
2024-01-25 06:01:42,381 - mmdet - INFO - Iter [2400/11160]	lr: 1.000e-02, eta: 3:26:44, time: 1.349, data_time: 0.015, memory: 11475, loss_cls: 0.7305, loss_bbox: 0.9569, loss: 1.6874, grad_norm: 1.3402
2024-01-25 06:02:49,704 - mmdet - INFO - Iter [2450/11160]	lr: 1.000e-02, eta: 3:25:21, time: 1.346, data_time: 0.015, memory: 11475, loss_cls: 0.7324, loss_bbox: 1.0198, loss: 1.7522, grad_norm: 1.4100
2024-01-25 06:03:56,351 - mmdet - INFO - Iter [2500/11160]	lr: 1.000e-02, eta: 3:23:56, time: 1.333, data_time: 0.015, memory: 11475, loss_cls: 0.7191, loss_bbox: 1.0077, loss: 1.7268, grad_norm: 1.4024
2024-01-25 06:05:04,369 - mmdet - INFO - Iter [2550/11160]	lr: 1.000e-02, eta: 3:22:37, time: 1.360, data_time: 0.016, memory: 11475, loss_cls: 0.7370, loss_bbox: 0.9609, loss: 1.6980, grad_norm: 1.5050
2024-01-25 06:06:12,197 - mmdet - INFO - Iter [2600/11160]	lr: 1.000e-02, eta: 3:21:17, time: 1.357, data_time: 0.016, memory: 11475, loss_cls: 0.7161, loss_bbox: 0.9701, loss: 1.6863, grad_norm: 1.4233
2024-01-25 06:07:19,818 - mmdet - INFO - Iter [2650/11160]	lr: 1.000e-02, eta: 3:19:57, time: 1.352, data_time: 0.016, memory: 11475, loss_cls: 0.7266, loss_bbox: 0.9683, loss: 1.6950, grad_norm: 1.3798
2024-01-25 06:08:28,042 - mmdet - INFO - Iter [2700/11160]	lr: 1.000e-02, eta: 3:18:40, time: 1.364, data_time: 0.016, memory: 11475, loss_cls: 0.7277, loss_bbox: 0.9545, loss: 1.6822, grad_norm: 1.3902
2024-01-25 06:09:35,338 - mmdet - INFO - Iter [2750/11160]	lr: 1.000e-02, eta: 3:17:19, time: 1.346, data_time: 0.016, memory: 11475, loss_cls: 0.7254, loss_bbox: 0.9531, loss: 1.6785, grad_norm: 1.4452
2024-01-25 06:10:43,006 - mmdet - INFO - Iter [2800/11160]	lr: 1.000e-02, eta: 3:16:01, time: 1.353, data_time: 0.016, memory: 11475, loss_cls: 0.7478, loss_bbox: 0.9728, loss: 1.7206, grad_norm: 1.6286
2024-01-25 06:11:50,351 - mmdet - INFO - Iter [2850/11160]	lr: 1.000e-02, eta: 3:14:42, time: 1.347, data_time: 0.016, memory: 11475, loss_cls: 0.7336, loss_bbox: 0.9466, loss: 1.6802, grad_norm: 1.4925
2024-01-25 06:12:57,585 - mmdet - INFO - Iter [2900/11160]	lr: 1.000e-02, eta: 3:13:23, time: 1.345, data_time: 0.016, memory: 11475, loss_cls: 0.7298, loss_bbox: 0.9802, loss: 1.7100, grad_norm: 1.5887
2024-01-25 06:14:05,201 - mmdet - INFO - Iter [2950/11160]	lr: 1.000e-02, eta: 3:12:05, time: 1.352, data_time: 0.016, memory: 11475, loss_cls: 0.7041, loss_bbox: 0.9435, loss: 1.6476, grad_norm: 1.3980
2024-01-25 06:15:12,986 - mmdet - INFO - Exp name: retinanet_26e.py
2024-01-25 06:15:12,987 - mmdet - INFO - Iter [3000/11160]	lr: 1.000e-02, eta: 3:10:49, time: 1.356, data_time: 0.016, memory: 11475, loss_cls: 0.6959, loss_bbox: 0.9708, loss: 1.6668, grad_norm: 1.4302
2024-01-25 06:16:20,855 - mmdet - INFO - Iter [3050/11160]	lr: 1.000e-02, eta: 3:09:32, time: 1.357, data_time: 0.016, memory: 11475, loss_cls: 0.7083, loss_bbox: 0.9773, loss: 1.6856, grad_norm: 1.5325
2024-01-25 06:17:27,764 - mmdet - INFO - Iter [3100/11160]	lr: 1.000e-02, eta: 3:08:14, time: 1.338, data_time: 0.016, memory: 11475, loss_cls: 0.7108, loss_bbox: 0.9133, loss: 1.6241, grad_norm: 1.5987
2024-01-25 06:18:34,861 - mmdet - INFO - Iter [3150/11160]	lr: 1.000e-02, eta: 3:06:56, time: 1.342, data_time: 0.016, memory: 11475, loss_cls: 0.7623, loss_bbox: 0.9569, loss: 1.7192, grad_norm: 1.7927
2024-01-25 06:19:42,177 - mmdet - INFO - Iter [3200/11160]	lr: 1.000e-02, eta: 3:05:40, time: 1.346, data_time: 0.016, memory: 11475, loss_cls: 0.7202, loss_bbox: 0.9442, loss: 1.6644, grad_norm: 1.5273
2024-01-25 06:20:50,033 - mmdet - INFO - Iter [3250/11160]	lr: 1.000e-02, eta: 3:04:24, time: 1.357, data_time: 0.016, memory: 11475, loss_cls: 0.7005, loss_bbox: 0.9518, loss: 1.6523, grad_norm: 1.4087
2024-01-25 06:21:57,557 - mmdet - INFO - Iter [3300/11160]	lr: 1.000e-02, eta: 3:03:09, time: 1.350, data_time: 0.016, memory: 11475, loss_cls: 0.6808, loss_bbox: 0.9474, loss: 1.6282, grad_norm: 1.3926
2024-01-25 06:23:16,742 - mmdet - INFO - Iter [3350/11160]	lr: 1.000e-02, eta: 3:13:01, time: 7.080, data_time: 5.726, memory: 11475, loss_cls: 0.7992, loss_bbox: 0.9559, loss: 1.7551, grad_norm: 1.5633
2024-01-25 06:24:25,003 - mmdet - INFO - Iter [3400/11160]	lr: 1.000e-02, eta: 3:11:34, time: 1.365, data_time: 0.015, memory: 11475, loss_cls: 0.8611, loss_bbox: 0.9883, loss: 1.8494, grad_norm: 1.8166
2024-01-25 06:25:32,226 - mmdet - INFO - Iter [3450/11160]	lr: 1.000e-02, eta: 3:10:04, time: 1.344, data_time: 0.015, memory: 11475, loss_cls: 0.8181, loss_bbox: 0.9673, loss: 1.7854, grad_norm: 1.7306
2024-01-25 06:26:39,814 - mmdet - INFO - Iter [3500/11160]	lr: 1.000e-02, eta: 3:08:36, time: 1.352, data_time: 0.016, memory: 11475, loss_cls: 0.7652, loss_bbox: 0.9426, loss: 1.7077, grad_norm: 1.6338
2024-01-25 06:27:46,916 - mmdet - INFO - Iter [3550/11160]	lr: 1.000e-02, eta: 3:07:08, time: 1.342, data_time: 0.016, memory: 11475, loss_cls: 0.7303, loss_bbox: 0.9633, loss: 1.6937, grad_norm: 1.6724
2024-01-25 06:28:54,270 - mmdet - INFO - Iter [3600/11160]	lr: 1.000e-02, eta: 3:05:41, time: 1.347, data_time: 0.016, memory: 11475, loss_cls: 0.7316, loss_bbox: 0.9339, loss: 1.6655, grad_norm: 1.7598
2024-01-25 06:30:01,479 - mmdet - INFO - Iter [3650/11160]	lr: 1.000e-02, eta: 3:04:14, time: 1.344, data_time: 0.016, memory: 11475, loss_cls: 0.7328, loss_bbox: 0.9264, loss: 1.6592, grad_norm: 1.7648
2024-01-25 06:31:09,177 - mmdet - INFO - Iter [3700/11160]	lr: 1.000e-02, eta: 3:02:48, time: 1.354, data_time: 0.016, memory: 11475, loss_cls: 0.7004, loss_bbox: 0.9534, loss: 1.6538, grad_norm: 1.7057
2024-01-25 06:32:16,038 - mmdet - INFO - Iter [3750/11160]	lr: 1.000e-02, eta: 3:01:22, time: 1.337, data_time: 0.016, memory: 11475, loss_cls: 0.7092, loss_bbox: 0.9351, loss: 1.6444, grad_norm: 1.5753
2024-01-25 06:33:23,093 - mmdet - INFO - Iter [3800/11160]	lr: 1.000e-02, eta: 2:59:56, time: 1.341, data_time: 0.016, memory: 11475, loss_cls: 0.6925, loss_bbox: 0.9466, loss: 1.6391, grad_norm: 1.6827
2024-01-25 06:34:30,450 - mmdet - INFO - Iter [3850/11160]	lr: 1.000e-02, eta: 2:58:31, time: 1.347, data_time: 0.016, memory: 11475, loss_cls: 0.6999, loss_bbox: 0.9297, loss: 1.6296, grad_norm: 1.6116
2024-01-25 06:35:37,778 - mmdet - INFO - Iter [3900/11160]	lr: 1.000e-02, eta: 2:57:07, time: 1.347, data_time: 0.016, memory: 11475, loss_cls: 0.7242, loss_bbox: 0.9426, loss: 1.6668, grad_norm: 1.8669
2024-01-25 06:36:43,468 - mmdet - INFO - Iter [3950/11160]	lr: 1.000e-02, eta: 2:55:40, time: 1.314, data_time: 0.016, memory: 11475, loss_cls: 0.7091, loss_bbox: 0.9077, loss: 1.6169, grad_norm: 1.6078
2024-01-25 06:37:51,180 - mmdet - INFO - Exp name: retinanet_26e.py
2024-01-25 06:37:51,181 - mmdet - INFO - Iter [4000/11160]	lr: 1.000e-02, eta: 2:54:17, time: 1.354, data_time: 0.016, memory: 11475, loss_cls: 0.7633, loss_bbox: 0.9511, loss: 1.7144, grad_norm: 1.6695
2024-01-25 06:38:57,829 - mmdet - INFO - Iter [4050/11160]	lr: 1.000e-02, eta: 2:52:53, time: 1.333, data_time: 0.016, memory: 11475, loss_cls: 0.6889, loss_bbox: 0.9465, loss: 1.6353, grad_norm: 1.3427
2024-01-25 06:40:05,229 - mmdet - INFO - Iter [4100/11160]	lr: 1.000e-02, eta: 2:51:30, time: 1.348, data_time: 0.016, memory: 11475, loss_cls: 0.7103, loss_bbox: 0.9010, loss: 1.6113, grad_norm: 1.5483
2024-01-25 06:41:12,634 - mmdet - INFO - Iter [4150/11160]	lr: 1.000e-02, eta: 2:50:08, time: 1.348, data_time: 0.016, memory: 11475, loss_cls: 0.6943, loss_bbox: 0.9312, loss: 1.6255, grad_norm: 1.7204
2024-01-25 06:42:18,238 - mmdet - INFO - Iter [4200/11160]	lr: 1.000e-02, eta: 2:48:43, time: 1.312, data_time: 0.016, memory: 11475, loss_cls: 0.6832, loss_bbox: 0.9627, loss: 1.6459, grad_norm: 1.6258
2024-01-25 06:43:25,415 - mmdet - INFO - Iter [4250/11160]	lr: 1.000e-02, eta: 2:47:22, time: 1.344, data_time: 0.016, memory: 11475, loss_cls: 0.6912, loss_bbox: 0.9440, loss: 1.6352, grad_norm: 1.6775
2024-01-25 06:44:33,109 - mmdet - INFO - Iter [4300/11160]	lr: 1.000e-02, eta: 2:46:01, time: 1.354, data_time: 0.016, memory: 11475, loss_cls: 0.7230, loss_bbox: 0.9431, loss: 1.6661, grad_norm: 1.6908
2024-01-25 06:45:40,178 - mmdet - INFO - Iter [4350/11160]	lr: 1.000e-02, eta: 2:44:40, time: 1.341, data_time: 0.016, memory: 11475, loss_cls: 0.6851, loss_bbox: 0.9165, loss: 1.6016, grad_norm: 1.5876
2024-01-25 06:46:47,048 - mmdet - INFO - Iter [4400/11160]	lr: 1.000e-02, eta: 2:43:19, time: 1.337, data_time: 0.016, memory: 11475, loss_cls: 0.7013, loss_bbox: 0.8868, loss: 1.5880, grad_norm: 1.5987
2024-01-25 06:47:54,786 - mmdet - INFO - Iter [4450/11160]	lr: 1.000e-02, eta: 2:41:59, time: 1.355, data_time: 0.016, memory: 11475, loss_cls: 0.6806, loss_bbox: 0.8927, loss: 1.5733, grad_norm: 1.6084
2024-01-25 06:49:14,333 - mmdet - INFO - Iter [4500/11160]	lr: 1.000e-02, eta: 2:41:02, time: 1.660, data_time: 0.333, memory: 11475, loss_cls: 0.6799, loss_bbox: 0.9536, loss: 1.6335, grad_norm: 1.6936
2024-01-25 06:50:21,533 - mmdet - INFO - Iter [4550/11160]	lr: 1.000e-02, eta: 2:39:42, time: 1.344, data_time: 0.015, memory: 11475, loss_cls: 0.6760, loss_bbox: 0.9086, loss: 1.5846, grad_norm: 1.7577
2024-01-25 06:51:28,730 - mmdet - INFO - Iter [4600/11160]	lr: 1.000e-02, eta: 2:38:22, time: 1.344, data_time: 0.016, memory: 11475, loss_cls: 0.7001, loss_bbox: 0.9375, loss: 1.6375, grad_norm: 1.6776
2024-01-25 06:52:35,576 - mmdet - INFO - Iter [4650/11160]	lr: 1.000e-02, eta: 2:37:02, time: 1.337, data_time: 0.016, memory: 11475, loss_cls: 0.7066, loss_bbox: 0.9415, loss: 1.6482, grad_norm: 1.6736
2024-01-25 06:53:42,794 - mmdet - INFO - Iter [4700/11160]	lr: 1.000e-02, eta: 2:35:42, time: 1.344, data_time: 0.016, memory: 11475, loss_cls: 0.6579, loss_bbox: 0.8819, loss: 1.5398, grad_norm: 1.5900
2024-01-25 06:54:49,963 - mmdet - INFO - Iter [4750/11160]	lr: 1.000e-02, eta: 2:34:23, time: 1.343, data_time: 0.016, memory: 11475, loss_cls: 0.8176, loss_bbox: 0.9453, loss: 1.7629, grad_norm: 2.0911
2024-01-25 06:55:57,323 - mmdet - INFO - Iter [4800/11160]	lr: 1.000e-02, eta: 2:33:04, time: 1.347, data_time: 0.016, memory: 11475, loss_cls: 0.7077, loss_bbox: 0.9592, loss: 1.6668, grad_norm: 1.4957
2024-01-25 06:57:04,702 - mmdet - INFO - Iter [4850/11160]	lr: 1.000e-02, eta: 2:31:46, time: 1.348, data_time: 0.016, memory: 11475, loss_cls: 0.6812, loss_bbox: 0.9264, loss: 1.6076, grad_norm: 1.6559
2024-01-25 06:58:11,503 - mmdet - INFO - Iter [4900/11160]	lr: 1.000e-02, eta: 2:30:27, time: 1.336, data_time: 0.017, memory: 11475, loss_cls: 0.6809, loss_bbox: 0.9204, loss: 1.6013, grad_norm: 1.6529
2024-01-25 06:59:18,365 - mmdet - INFO - Iter [4950/11160]	lr: 1.000e-02, eta: 2:29:08, time: 1.337, data_time: 0.016, memory: 11475, loss_cls: 0.6879, loss_bbox: 0.9351, loss: 1.6230, grad_norm: 1.7621
2024-01-25 07:00:25,577 - mmdet - INFO - Exp name: retinanet_26e.py
2024-01-25 07:00:25,578 - mmdet - INFO - Iter [5000/11160]	lr: 1.000e-02, eta: 2:27:50, time: 1.344, data_time: 0.016, memory: 11475, loss_cls: 0.6937, loss_bbox: 0.9399, loss: 1.6336, grad_norm: 1.6460
2024-01-25 07:01:32,938 - mmdet - INFO - Iter [5050/11160]	lr: 1.000e-02, eta: 2:26:32, time: 1.347, data_time: 0.016, memory: 11475, loss_cls: 0.7659, loss_bbox: 0.9244, loss: 1.6903, grad_norm: 1.7320
2024-01-25 07:02:39,404 - mmdet - INFO - Iter [5100/11160]	lr: 1.000e-02, eta: 2:25:14, time: 1.329, data_time: 0.016, memory: 11475, loss_cls: 0.6736, loss_bbox: 0.8761, loss: 1.5497, grad_norm: 1.5361
2024-01-25 07:03:46,724 - mmdet - INFO - Iter [5150/11160]	lr: 1.000e-02, eta: 2:23:57, time: 1.346, data_time: 0.016, memory: 11475, loss_cls: 0.6728, loss_bbox: 0.9020, loss: 1.5748, grad_norm: 1.6082
2024-01-25 07:04:53,178 - mmdet - INFO - Iter [5200/11160]	lr: 1.000e-02, eta: 2:22:39, time: 1.329, data_time: 0.016, memory: 11475, loss_cls: 0.6742, loss_bbox: 0.9054, loss: 1.5796, grad_norm: 1.7272
2024-01-25 07:06:00,401 - mmdet - INFO - Iter [5250/11160]	lr: 1.000e-02, eta: 2:21:22, time: 1.344, data_time: 0.016, memory: 11475, loss_cls: 0.7198, loss_bbox: 0.9423, loss: 1.6621, grad_norm: 1.9411
2024-01-25 07:07:08,156 - mmdet - INFO - Iter [5300/11160]	lr: 1.000e-02, eta: 2:20:05, time: 1.355, data_time: 0.016, memory: 11475, loss_cls: 0.6665, loss_bbox: 0.8887, loss: 1.5552, grad_norm: 1.3608
2024-01-25 07:08:16,012 - mmdet - INFO - Iter [5350/11160]	lr: 1.000e-02, eta: 2:18:50, time: 1.357, data_time: 0.016, memory: 11475, loss_cls: 0.7168, loss_bbox: 0.9045, loss: 1.6213, grad_norm: 1.6516
2024-01-25 07:09:23,263 - mmdet - INFO - Iter [5400/11160]	lr: 1.000e-02, eta: 2:17:33, time: 1.345, data_time: 0.016, memory: 11475, loss_cls: 0.6722, loss_bbox: 0.8879, loss: 1.5601, grad_norm: 1.5812
2024-01-25 07:10:30,596 - mmdet - INFO - Iter [5450/11160]	lr: 1.000e-02, eta: 2:16:17, time: 1.347, data_time: 0.016, memory: 11475, loss_cls: 0.6740, loss_bbox: 0.8927, loss: 1.5667, grad_norm: 1.6314
2024-01-25 07:11:38,409 - mmdet - INFO - Iter [5500/11160]	lr: 1.000e-02, eta: 2:15:01, time: 1.356, data_time: 0.016, memory: 11475, loss_cls: 0.6688, loss_bbox: 0.9122, loss: 1.5809, grad_norm: 1.7213
2024-01-25 07:12:46,386 - mmdet - INFO - Iter [5550/11160]	lr: 1.000e-02, eta: 2:13:46, time: 1.360, data_time: 0.016, memory: 11475, loss_cls: 0.7070, loss_bbox: 0.8858, loss: 1.5928, grad_norm: 1.6761
2024-01-25 07:13:27,978 - mmdet - INFO - Saving checkpoint at 5 epochs
2024-01-25 07:14:08,731 - mmdet - INFO - Iter [5600/11160]	lr: 1.000e-02, eta: 2:13:02, time: 1.986, data_time: 0.641, memory: 11475, loss_cls: 0.6706, loss_bbox: 0.9263, loss: 1.5969, grad_norm: 1.6614
2024-01-25 07:15:16,873 - mmdet - INFO - Iter [5650/11160]	lr: 1.000e-02, eta: 2:11:47, time: 1.363, data_time: 0.014, memory: 11475, loss_cls: 0.6527, loss_bbox: 0.8950, loss: 1.5476, grad_norm: 1.7168
2024-01-25 07:16:22,432 - mmdet - INFO - Iter [5700/11160]	lr: 1.000e-02, eta: 2:10:29, time: 1.311, data_time: 0.014, memory: 11475, loss_cls: 0.6610, loss_bbox: 0.8944, loss: 1.5554, grad_norm: 1.7305
2024-01-25 07:17:30,265 - mmdet - INFO - Iter [5750/11160]	lr: 1.000e-02, eta: 2:09:14, time: 1.357, data_time: 0.016, memory: 11475, loss_cls: 0.6529, loss_bbox: 0.8523, loss: 1.5052, grad_norm: 1.7034
2024-01-25 07:18:37,554 - mmdet - INFO - Iter [5800/11160]	lr: 1.000e-02, eta: 2:07:58, time: 1.346, data_time: 0.015, memory: 11475, loss_cls: 0.6429, loss_bbox: 0.8838, loss: 1.5267, grad_norm: 1.5081
2024-01-25 07:19:44,628 - mmdet - INFO - Iter [5850/11160]	lr: 1.000e-02, eta: 2:06:42, time: 1.341, data_time: 0.015, memory: 11475, loss_cls: 0.6656, loss_bbox: 0.8870, loss: 1.5526, grad_norm: 1.7908
